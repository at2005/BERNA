{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "n_embed = 128\n",
    "n_genes = 500\n",
    "batch_size = 64\n",
    "time_dim = 20\n",
    "n_heads = 8\n",
    "dropout = 0\n",
    "n_perts = 2\n",
    "n_blocks = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import scanpy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "adata = sc.read(\"/Users/atamb/Downloads/NormanWeissman2019_filtered.h5ad\")\n",
    "# normalise data\n",
    "sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "# log transform the data\n",
    "sc.pp.log1p(adata)\n",
    "\n",
    "adata_subset = adata[:,:n_genes]\n",
    "# prep dataset\n",
    "adata_subset.X\n",
    "\n",
    "dataset = torch.tensor(adata_subset.X.toarray())\n",
    "top_vals, top_inds = torch.topk(dataset, time_dim, dim=1)\n",
    "top_inds = top_inds.to(device=\"mps\")\n",
    "\n",
    "\n",
    "# top_inds.to(device=\"mps\")\n",
    "\n",
    "gene_lst = list(adata_subset.var_names)\n",
    "gene_lst.append(\"control\")\n",
    "\n",
    "pertlst = list(adata_subset.obs.perturbation)\n",
    "len(pertlst)\n",
    "\n",
    "pert_dataset = []\n",
    "\n",
    "def gene_encoder(x):\n",
    "    x = x.split(\"_\")\n",
    "    if len(x) == 1:\n",
    "        # add one in order to account for mask being at index 0\n",
    "        return [gene_lst.index(x[0]) + 1, gene_lst.index(\"control\") + 1]\n",
    "    else:\n",
    "        return [gene_lst.index(x[0]) + 1, gene_lst.index(x[1]) + 1]\n",
    "\n",
    "\n",
    "for pert in pertlst:\n",
    "    ls = pert.split(\"_\")\n",
    "    if ls[0] not in gene_lst:\n",
    "        gene_lst.append(ls[0])\n",
    "    if len(ls) > 1:\n",
    "        if ls[1] not in gene_lst:\n",
    "            gene_lst.append(ls[1])\n",
    "    \n",
    "    pert_dataset.append(gene_encoder(pert))\n",
    "\n",
    "    # print(gene_encoder(pert))\n",
    "\n",
    "pert_dataset_tensor = torch.tensor(pert_dataset, device=\"mps\")\n",
    "\n",
    "n_genes = len(gene_lst)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_batch():\n",
    "    index_tensor = torch.randint(0, len(pert_dataset_tensor), (batch_size,), device=\"mps\")\n",
    "    # print(index_tensor)\n",
    "    perts = pert_dataset_tensor[index_tensor]\n",
    "    resp = top_inds[index_tensor]\n",
    "    return perts, resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# block_sizes = [128, 64, 32, 16]\n",
    "mask_factor = 0.2\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, n_embed, head_size):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        self.batch_qkv_matrices = nn.Linear(n_embed, head_size * n_heads * 3, bias=False) \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        q,k,v = self.batch_qkv_matrices(x).split(self.head_size * n_heads, dim=-1) # Now Q,K,V of dim B, T, head size * n_heads\n",
    "\n",
    "        B,T,C = x.shape\n",
    " \n",
    "        # reshape to B, T, n_heads, head_size\n",
    "        k = k.view(B, T, n_heads, self.head_size).transpose(1,2)\n",
    "        q = q.view(B, T, n_heads, self.head_size).transpose(1,2) # Now of shape B, n_heads, T, head_size for BMM\n",
    "        v = v.view(B, T, n_heads, self.head_size).transpose(1,2)\n",
    "   \n",
    "        # attention mechanism core\n",
    "        weight_mat = q @ k.transpose(-2, -1)\n",
    "        weight_mat = weight_mat * (self.head_size ** -0.5) #\n",
    "        weight_mat = F.softmax(weight_mat, dim=-1)\n",
    "\n",
    "        # regularisation\n",
    "        weight_mat = self.dropout(weight_mat)\n",
    "\n",
    "        # Multiply with values\n",
    "        res = weight_mat @ v\n",
    "\n",
    "        # post-processing\n",
    "        res = res.transpose(1,2) # B, n_heads, T, C --> B, T, n_heads, C   \n",
    "        res = res.contiguous().view(B, T, C)\n",
    "\n",
    "        return res\n",
    "\n",
    "\n",
    "class MHAttention(nn.Module):\n",
    "    def __init__(self, n_embed, head_size):\n",
    "        super().__init__()\n",
    "        self.att_heads = Head(n_embed=n_embed, head_size=head_size)\n",
    "        self.projection = nn.Linear(n_embed, n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        res = self.att_heads(x)\n",
    "        res = self.dropout(self.projection(res))\n",
    "        return res \n",
    "\n",
    "class Feedforward(nn.Module):\n",
    "    def __init__(self, n_embed) -> None:\n",
    "        super().__init__()\n",
    "        scale_factor = 6\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(n_embed, n_embed * scale_factor),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_embed * scale_factor, n_embed),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_embed, n_embed*scale_factor),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_embed*scale_factor, n_embed),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ff(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embed, n_heads) -> None:\n",
    "        super().__init__()\n",
    "        self.ff = Feedforward(n_embed)\n",
    "        self.mhatt = MHAttention(n_embed, (n_embed // n_heads))\n",
    "        self.layer_norm1 = nn.LayerNorm(n_embed) \n",
    "        self.layer_norm2 = nn.LayerNorm(n_embed)\n",
    "    def forward(self, x):\n",
    "        x = x + self.mhatt(self.layer_norm1(x))\n",
    "        x = x + self.ff(self.layer_norm2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "    \n",
    "# Bidirectional Encoder representations from transformers for RNA-seq (BERNA)\n",
    "class BERNA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERNA, self).__init__()\n",
    "        # 0th embedding is for mask!!!\n",
    "        self.embed_table = nn.Embedding(n_genes+1, n_embed)\n",
    "        # two embeddings, unique for both perts and responses\n",
    "        self.pos_embed = nn.Embedding(2, n_embed)\n",
    "        # we want to just keep the indices for positional embeddings stored so we don't create a new tensor every time\n",
    "        self.register_buffer(\"perts_pos_embed\", torch.zeros(size=(batch_size,n_perts), device=\"mps\").long())\n",
    "        self.register_buffer(\"responses_pos_embed\", torch.ones(size=(batch_size,time_dim), device=\"mps\").long())\n",
    "\n",
    "        # Attention blocks\n",
    "        self.blocks = nn.Sequential(*[Block(n_embed, n_heads) for _ in range(n_blocks)])\n",
    "        self.layernorm = nn.LayerNorm(n_embed)\n",
    "        self.final_proj = nn.Linear(n_embed, n_genes)\n",
    "\n",
    "\n",
    "    def forward(self,perts, responses):\n",
    "        x = torch.cat([perts, responses], dim=-1) # of dimension (B, 12). 12 because 2 perts and 10 responses per batch element\n",
    "        # get ones, then find random positions to mask, set to zero, then multiply\n",
    "\n",
    "        # generate mask\n",
    "        mask = torch.ones_like(x)\n",
    "        # creation of random positions to mask\n",
    "        rand_pos = torch.randint(0, time_dim + n_perts, size=(batch_size, int(x.shape[-1]*mask_factor) ), device=\"mps\")\n",
    "        # gather all positions to mask, ie the target tokens which we want to predict\n",
    "        # print(x.shape)\n",
    "        target_token = torch.gather(x, 1, rand_pos)\n",
    "        # set all positions we wish to predict in x to zero, is [MASK] token\n",
    "        mask.scatter_(1, rand_pos, 0)\n",
    "        x = x * mask\n",
    "\n",
    "        # # re-split after masking into perts and responses\n",
    "        perts_mod = x[:, :n_perts]\n",
    "        responses_mod = x[:, n_perts:]\n",
    "\n",
    "        # get perturbation embeddings\n",
    "        perts_embed = self.embed_table(perts_mod) + self.pos_embed(self.perts_pos_embed)\n",
    "\n",
    "        # get response embeddings\n",
    "        responses_embed = self.embed_table(responses_mod) + self.pos_embed(self.responses_pos_embed)\n",
    "\n",
    "        # re-concatenate and pass through blocks\n",
    "        x = torch.cat([perts_embed, responses_embed], dim=1)\n",
    "        x = self.final_proj(self.layernorm(self.blocks(x)))\n",
    "\n",
    "        # ensure that rand_pos is same shape as x\n",
    "        modified_rand_pos = rand_pos.unsqueeze(-1).expand(-1,-1,x.shape[-1])\n",
    "        logits = torch.gather(x, 1, modified_rand_pos)\n",
    "        B,T,C = logits.shape\n",
    "        logits = logits.view(B*T, C)\n",
    "        target_token = target_token.view(B*T)\n",
    "        loss = F.cross_entropy(logits, target_token)        \n",
    "        return loss        \n",
    "\n",
    "\n",
    "# b = BERNA().to(device=\"mps\")\n",
    "# test_perts = torch.randint(0, n_genes, (batch_size,2,)).to(device=\"mps\")\n",
    "# test_responses = torch.randint(0, n_genes, (batch_size,10)).to(device=\"mps\")\n",
    "# resp = b(test_perts, test_responses)\n",
    "# print(resp)\n",
    "# resp.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERNA().to(device=\"mps\")\n",
    "optim = torch.optim.Adam(model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.65017032623291\n",
      "5.094583158493042\n",
      "4.221706733703614\n",
      "3.76480459690094\n",
      "3.6407118749618532\n",
      "3.5485582733154297\n",
      "3.5103693056106566\n",
      "3.46583740234375\n",
      "3.4792668771743775\n",
      "3.436477928161621\n",
      "3.4386022424697877\n",
      "3.4280880975723265\n",
      "3.409325571060181\n",
      "3.441022834777832\n",
      "3.4124304246902466\n",
      "3.382338213920593\n",
      "3.4216264963150023\n",
      "3.381232476234436\n",
      "3.3870992183685305\n",
      "3.3705470514297486\n",
      "3.350314302444458\n",
      "3.373988699913025\n",
      "3.3576741361618043\n",
      "3.3615059900283812\n",
      "3.3876878595352173\n",
      "3.349146499633789\n",
      "3.33632559299469\n",
      "3.3634633827209472\n",
      "3.358061981201172\n",
      "3.3396395111083983\n",
      "3.356384139060974\n",
      "3.3108683109283445\n",
      "3.315969452857971\n",
      "3.3217591857910156\n",
      "3.3273305559158324\n",
      "3.327773718833923\n",
      "3.340449113845825\n",
      "3.3109439611434937\n",
      "3.341012692451477\n",
      "3.3267406368255616\n",
      "3.318300557136536\n",
      "3.345765314102173\n",
      "3.314155926704407\n",
      "3.309562129974365\n",
      "3.3262577152252195\n",
      "3.292046389579773\n",
      "3.298604898452759\n",
      "3.2796576976776124\n",
      "3.310287389755249\n",
      "3.2892987298965455\n",
      "3.2725835990905763\n",
      "3.274227681159973\n",
      "3.2691804933547974\n",
      "3.2929777479171753\n",
      "3.2688479232788086\n",
      "3.264308524131775\n",
      "3.2748201990127566\n",
      "3.2646149826049804\n",
      "3.260867133140564\n",
      "3.250564455986023\n",
      "3.237449278831482\n",
      "3.2446540784835816\n",
      "3.252876443862915\n",
      "3.286308250427246\n",
      "3.23758252620697\n",
      "3.242064776420593\n",
      "3.232247052192688\n",
      "3.2645277786254883\n",
      "3.244051055908203\n",
      "3.216135234832764\n",
      "3.2311597442626954\n",
      "3.2368577337265014\n",
      "3.250390305519104\n",
      "3.2164687728881836\n",
      "3.230694279670715\n",
      "3.2349632215499877\n",
      "3.230751647949219\n",
      "3.2512606143951417\n",
      "3.234327025413513\n",
      "3.2607802104949952\n",
      "3.2556598377227783\n",
      "3.237765460014343\n",
      "3.2349631118774416\n",
      "3.236126117706299\n",
      "3.222690978050232\n",
      "3.2328836870193483\n",
      "3.2612956047058104\n",
      "3.246200532913208\n",
      "3.2044640827178954\n",
      "3.264088068008423\n",
      "3.2233583164215087\n",
      "3.253342514038086\n",
      "3.23425133228302\n",
      "3.2352967500686645\n",
      "3.2336262464523315\n",
      "3.2241548109054565\n",
      "3.205556902885437\n",
      "3.2188657569885253\n",
      "3.2284798574447633\n",
      "3.1984492254257204\n",
      "3.187023983001709\n",
      "3.241055688858032\n",
      "3.2312599754333498\n",
      "3.240570206642151\n",
      "3.2335084438323975\n",
      "3.2311979961395263\n",
      "3.2242875623703005\n",
      "3.24941388130188\n",
      "3.203360872268677\n",
      "3.232202548980713\n",
      "3.1992490911483764\n",
      "3.2211546087265015\n",
      "3.2334374523162843\n",
      "3.202449679374695\n",
      "3.1989606475830077\n",
      "3.2062441730499267\n",
      "3.2056438302993775\n",
      "3.1863412618637086\n",
      "3.222816815376282\n",
      "3.216324496269226\n",
      "3.203093490600586\n",
      "3.2319898653030394\n",
      "3.216326208114624\n",
      "3.164827837944031\n",
      "3.257468371391296\n",
      "3.2107684659957885\n",
      "3.2034405422210694\n",
      "3.1913112020492553\n",
      "3.2316543865203857\n",
      "3.217047643661499\n",
      "3.217347812652588\n",
      "3.202713842391968\n",
      "3.217939643859863\n",
      "3.2222520780563353\n",
      "3.2422840356826783\n",
      "3.2313985633850097\n",
      "3.2211022996902465\n",
      "3.1807068967819214\n",
      "3.198479232788086\n",
      "3.197624936103821\n",
      "3.1909940385818483\n",
      "3.195301027297974\n",
      "3.195823826789856\n",
      "3.182233076095581\n",
      "3.2311031723022463\n",
      "3.205601944923401\n",
      "3.2126371908187865\n",
      "3.170508303642273\n",
      "3.2117999458312987\n",
      "3.190767741203308\n",
      "3.1932041597366334\n",
      "3.2055055809021\n",
      "3.2300292015075684\n",
      "3.194351978302002\n",
      "3.216090803146362\n",
      "3.1940001106262206\n",
      "3.2034136962890627\n",
      "3.2248012733459475\n",
      "3.193286256790161\n",
      "3.2085313749313356\n",
      "3.1973450708389284\n",
      "3.195286960601807\n",
      "3.2119096374511718\n",
      "3.1931083583831787\n",
      "3.2093254470825197\n",
      "3.2138557386398316\n",
      "3.2098797225952147\n",
      "3.194287939071655\n",
      "3.2159866762161253\n",
      "3.1814464473724366\n",
      "3.2324144077301025\n",
      "3.2049697160720827\n",
      "3.207864274978638\n",
      "3.211331877708435\n",
      "3.211554265022278\n",
      "3.1845694971084595\n",
      "3.213237943649292\n",
      "3.21223268032074\n",
      "3.1925999402999876\n",
      "3.197249970436096\n",
      "3.2111286544799804\n",
      "3.20794141292572\n",
      "3.2096295833587645\n",
      "3.2068714904785156\n",
      "3.203266181945801\n",
      "3.172863941192627\n",
      "3.1899767875671388\n",
      "3.2254926538467408\n",
      "3.2041498470306395\n",
      "3.185908098220825\n",
      "3.1806954956054687\n",
      "3.1983421802520753\n",
      "3.171555814743042\n",
      "3.224959902763367\n",
      "3.173694162368774\n",
      "3.211322612762451\n",
      "3.2107308959960936\n",
      "3.193412251472473\n",
      "3.2111733245849607\n",
      "3.1779929208755493\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10000\n",
    "\n",
    "loss_lst = []\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    optim.zero_grad()\n",
    "    perts, resp = gen_batch()\n",
    "    loss = model(perts, resp)\n",
    "    loss.backward()\n",
    "    loss_lst.append(loss.item())\n",
    "    if i % 50 == 0:\n",
    "        print(sum(loss_lst) / len(loss_lst))\n",
    "        loss_lst = []\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save weights\n",
    "torch.save(model.state_dict(), \"berna.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 501]\n",
      "torch.Size([1, 2, 128])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    # get embeddings\n",
    "    token = gene_encoder(\"OR4F5\")\n",
    "    print(token)\n",
    "    vec = model.embed_table(torch.tensor(token, device=\"mps\").unsqueeze(0))\n",
    "    print(vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 501]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gene_encoder(\"RP11-34P13.7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RP11-34P13.3',\n",
       " 'FAM138A',\n",
       " 'OR4F5',\n",
       " 'RP11-34P13.7',\n",
       " 'RP11-34P13.8',\n",
       " 'RP11-34P13.14',\n",
       " 'RP11-34P13.9',\n",
       " 'FO538757.3',\n",
       " 'FO538757.2',\n",
       " 'AP006222.2',\n",
       " 'RP5-857K21.15',\n",
       " 'RP4-669L17.2',\n",
       " 'RP4-669L17.10',\n",
       " 'OR4F29',\n",
       " 'RP5-857K21.4',\n",
       " 'RP5-857K21.2',\n",
       " 'OR4F16',\n",
       " 'RP11-206L10.4',\n",
       " 'RP11-206L10.9',\n",
       " 'FAM87B',\n",
       " 'LINC00115',\n",
       " 'FAM41C',\n",
       " 'RP11-54O7.16',\n",
       " 'RP11-54O7.1',\n",
       " 'RP11-54O7.2',\n",
       " 'RP11-54O7.3',\n",
       " 'SAMD11',\n",
       " 'NOC2L',\n",
       " 'KLHL17',\n",
       " 'PLEKHN1',\n",
       " 'PERM1',\n",
       " 'RP11-54O7.17',\n",
       " 'HES4',\n",
       " 'ISG15',\n",
       " 'RP11-54O7.11',\n",
       " 'AGRN',\n",
       " 'RP11-54O7.18',\n",
       " 'RNF223',\n",
       " 'C1orf159',\n",
       " 'LINC01342',\n",
       " 'RP11-465B22.8',\n",
       " 'TTLL10-AS1',\n",
       " 'TTLL10',\n",
       " 'TNFRSF18',\n",
       " 'TNFRSF4',\n",
       " 'SDF4',\n",
       " 'B3GALT6',\n",
       " 'FAM132A',\n",
       " 'RP5-902P8.12',\n",
       " 'UBE2J2',\n",
       " 'RP5-902P8.10',\n",
       " 'SCNN1D',\n",
       " 'ACAP3',\n",
       " 'PUSL1',\n",
       " 'CPSF3L',\n",
       " 'CPTP',\n",
       " 'TAS1R3',\n",
       " 'DVL1',\n",
       " 'MXRA8',\n",
       " 'AURKAIP1',\n",
       " 'CCNL2',\n",
       " 'MRPL20',\n",
       " 'RP4-758J18.13',\n",
       " 'ANKRD65',\n",
       " 'RP4-758J18.7',\n",
       " 'TMEM88B',\n",
       " 'RP4-758J18.10',\n",
       " 'VWA1',\n",
       " 'ATAD3C',\n",
       " 'ATAD3B',\n",
       " 'ATAD3A',\n",
       " 'TMEM240',\n",
       " 'SSU72',\n",
       " 'RP5-832C2.5',\n",
       " 'AL645728.1',\n",
       " 'C1orf233',\n",
       " 'RP11-345P4.9',\n",
       " 'MIB2',\n",
       " 'MMP23B',\n",
       " 'CDK11B',\n",
       " 'RP11-345P4.10',\n",
       " 'SLC35E2B',\n",
       " 'RP11-345P4.7',\n",
       " 'CDK11A',\n",
       " 'SLC35E2',\n",
       " 'NADK',\n",
       " 'GNB1',\n",
       " 'RP1-140A9.1',\n",
       " 'CALML6',\n",
       " 'TMEM52',\n",
       " 'CFAP74',\n",
       " 'RP11-547D24.1',\n",
       " 'GABRD',\n",
       " 'RP11-547D24.3',\n",
       " 'PRKCZ',\n",
       " 'RP5-892K4.1',\n",
       " 'RP11-181G12.2',\n",
       " 'FAAP20',\n",
       " 'RP11-181G12.4',\n",
       " 'SKI',\n",
       " 'MORN1',\n",
       " 'RER1',\n",
       " 'PEX10',\n",
       " 'PLCH2',\n",
       " 'RP3-395M20.2',\n",
       " 'RP3-395M20.3',\n",
       " 'PANK4',\n",
       " 'HES5',\n",
       " 'RP3-395M20.12',\n",
       " 'RP3-395M20.8',\n",
       " 'TNFRSF14',\n",
       " 'RP3-395M20.9',\n",
       " 'FAM213B',\n",
       " 'MMEL1',\n",
       " 'RP13-436F16.1',\n",
       " 'TTC34',\n",
       " 'RP11-740P5.2',\n",
       " 'RP11-740P5.3',\n",
       " 'ACTRT2',\n",
       " 'LINC00982',\n",
       " 'PRDM16',\n",
       " 'RP1-163G9.2',\n",
       " 'RP11-22L13.1',\n",
       " 'ARHGEF16',\n",
       " 'RP11-168F9.2',\n",
       " 'MEGF6',\n",
       " 'RP11-46F15.2',\n",
       " 'TPRG1L',\n",
       " 'WRAP73',\n",
       " 'TP73',\n",
       " 'RP5-1092A11.5',\n",
       " 'RP5-1092A11.2',\n",
       " 'TP73-AS1',\n",
       " 'CCDC27',\n",
       " 'SMIM1',\n",
       " 'LRRC47',\n",
       " 'RP1-286D6.5',\n",
       " 'CEP104',\n",
       " 'DFFB',\n",
       " 'C1orf174',\n",
       " 'LINC01134',\n",
       " 'LINC01346',\n",
       " 'RP13-614K11.2',\n",
       " 'RP5-1166F10.1',\n",
       " 'RP1-37J18.1',\n",
       " 'RP1-37J18.2',\n",
       " 'AJAP1',\n",
       " 'RP11-542C10.1',\n",
       " 'RP1-58B11.1',\n",
       " 'RP11-154H17.1',\n",
       " 'NPHP4',\n",
       " 'KCNAB2',\n",
       " 'CHD5',\n",
       " 'RPL22',\n",
       " 'RP1-120G22.11',\n",
       " 'RNF207',\n",
       " 'ICMT',\n",
       " 'LINC00337',\n",
       " 'HES3',\n",
       " 'GPR153',\n",
       " 'ACOT7',\n",
       " 'RP1-202O8.3',\n",
       " 'HES2',\n",
       " 'ESPN',\n",
       " 'RP1-202O8.2',\n",
       " 'TNFRSF25',\n",
       " 'PLEKHG5',\n",
       " 'NOL9',\n",
       " 'TAS1R1',\n",
       " 'ZBTB48',\n",
       " 'KLHL21',\n",
       " 'PHF13',\n",
       " 'THAP3',\n",
       " 'DNAJC11',\n",
       " 'RP11-242F24.1',\n",
       " 'RP11-312B8.1',\n",
       " 'CAMTA1',\n",
       " 'RP11-334N17.1',\n",
       " 'RP3-453P22.2',\n",
       " 'RP4-549F15.1',\n",
       " 'RP11-338N10.1',\n",
       " 'RP11-338N10.2',\n",
       " 'RP11-338N10.3',\n",
       " 'VAMP3',\n",
       " 'PER3',\n",
       " 'RP3-467L1.4',\n",
       " 'UTS2',\n",
       " 'TNFRSF9',\n",
       " 'PARK7',\n",
       " 'ERRFI1',\n",
       " 'RP11-431K24.1',\n",
       " 'RP11-431K24.3',\n",
       " 'RP11-431K24.4',\n",
       " 'SLC45A1',\n",
       " 'RERE',\n",
       " 'RP5-1115A15.1',\n",
       " 'RP4-633I8.4',\n",
       " 'ENO1',\n",
       " 'ENO1-AS1',\n",
       " 'CA6',\n",
       " 'SLC2A7',\n",
       " 'SLC2A5',\n",
       " 'GPR157',\n",
       " 'MIR34AHG',\n",
       " 'RP3-510D11.2',\n",
       " 'RP3-510D11.4',\n",
       " 'H6PD',\n",
       " 'SPSB1',\n",
       " 'RP13-392I16.1',\n",
       " 'SLC25A33',\n",
       " 'TMEM201',\n",
       " 'PIK3CD',\n",
       " 'PIK3CD-AS1',\n",
       " 'PIK3CD-AS2',\n",
       " 'CLSTN1',\n",
       " 'AL357140.1',\n",
       " 'CTNNBIP1',\n",
       " 'RP11-84A14.5',\n",
       " 'LZIC',\n",
       " 'NMNAT1',\n",
       " 'RBP7',\n",
       " 'UBE4B',\n",
       " 'KIF1B',\n",
       " 'PGD',\n",
       " 'RP4-736L20.3',\n",
       " 'APITD1-CORT',\n",
       " 'APITD1',\n",
       " 'CORT',\n",
       " 'DFFA',\n",
       " 'RP5-1113E3.3',\n",
       " 'PEX14',\n",
       " 'CASZ1',\n",
       " 'RP4-734G22.3',\n",
       " 'C1orf127',\n",
       " 'TARDBP',\n",
       " 'RP4-635E18.9',\n",
       " 'MASP2',\n",
       " 'RP4-635E18.8',\n",
       " 'SRM',\n",
       " 'EXOSC10',\n",
       " 'RP4-635E18.7',\n",
       " 'RP4-635E18.6',\n",
       " 'MTOR',\n",
       " 'MTOR-AS1',\n",
       " 'ANGPTL7',\n",
       " 'UBIAD1',\n",
       " 'PTCHD2',\n",
       " 'RP1-69M21.2',\n",
       " 'FBXO2',\n",
       " 'FBXO44',\n",
       " 'FBXO6',\n",
       " 'MAD2L2',\n",
       " 'DRAXIN',\n",
       " 'AGTRAP',\n",
       " 'C1orf167',\n",
       " 'RP11-56N19.5',\n",
       " 'MTHFR',\n",
       " 'CLCN6',\n",
       " 'NPPA-AS1',\n",
       " 'NPPA',\n",
       " 'NPPB',\n",
       " 'KIAA2013',\n",
       " 'PLOD1',\n",
       " 'MFN2',\n",
       " 'MIIP',\n",
       " 'TNFRSF8',\n",
       " 'TNFRSF1B',\n",
       " 'VPS13D',\n",
       " 'RP5-888M10.2',\n",
       " 'DHRS3',\n",
       " 'RP11-474O21.5',\n",
       " 'AADACL4',\n",
       " 'AADACL3',\n",
       " 'C1orf158',\n",
       " 'PRAMEF12',\n",
       " 'PRAMEF1',\n",
       " 'RP5-845O24.8',\n",
       " 'PRAMEF11',\n",
       " 'HNRNPCL1',\n",
       " 'PRAMEF2',\n",
       " 'PRAMEF4',\n",
       " 'PRAMEF10',\n",
       " 'PRAMEF7',\n",
       " 'PRAMEF6',\n",
       " 'PRAMEF27',\n",
       " 'PRAMEF26',\n",
       " 'PRAMEF25',\n",
       " 'HNRNPCL3',\n",
       " 'HNRNPCL2',\n",
       " 'RP13-221M14.2',\n",
       " 'HNRNPCL4',\n",
       " 'PRAMEF9',\n",
       " 'PRAMEF18',\n",
       " 'PRAMEF5',\n",
       " 'PRAMEF8',\n",
       " 'PRAMEF15',\n",
       " 'RP11-219C24.10',\n",
       " 'PRAMEF14',\n",
       " 'PRAMEF19',\n",
       " 'PRAMEF17',\n",
       " 'PRAMEF20',\n",
       " 'LRRC38',\n",
       " 'RP4-597A16.2',\n",
       " 'PDPN',\n",
       " 'CTA-520D8.2',\n",
       " 'PRDM2',\n",
       " 'RP11-344F13.1',\n",
       " 'RP4-704D23.1',\n",
       " 'KAZN',\n",
       " 'TMEM51-AS1',\n",
       " 'TMEM51',\n",
       " 'C1orf195',\n",
       " 'FHAD1',\n",
       " 'RP3-467K16.2',\n",
       " 'RP3-467K16.7',\n",
       " 'RP3-467K16.4',\n",
       " 'EFHD2',\n",
       " 'CTRC',\n",
       " 'CELA2A',\n",
       " 'CELA2B',\n",
       " 'CASP9',\n",
       " 'DNAJC16',\n",
       " 'RP4-680D5.8',\n",
       " 'AGMAT',\n",
       " 'RP4-680D5.2',\n",
       " 'DDI2',\n",
       " 'RSC1A1',\n",
       " 'RP4-680D5.9',\n",
       " 'PLEKHM2',\n",
       " 'RP11-288I21.1',\n",
       " 'SLC25A34',\n",
       " 'RP11-169K16.4',\n",
       " 'TMEM82',\n",
       " 'FBLIM1',\n",
       " 'UQCRHL',\n",
       " 'FLJ37453',\n",
       " 'SPEN',\n",
       " 'ZBTB17',\n",
       " 'C1orf64',\n",
       " 'RP11-5P18.5',\n",
       " 'HSPB7',\n",
       " 'CLCNKA',\n",
       " 'CLCNKB',\n",
       " 'FAM131C',\n",
       " 'EPHA2',\n",
       " 'RP11-276H7.2',\n",
       " 'RP11-276H7.3',\n",
       " 'ARHGEF19-AS1',\n",
       " 'ARHGEF19',\n",
       " 'RSG1',\n",
       " 'FBXO42',\n",
       " 'SZRD1',\n",
       " 'SPATA21',\n",
       " 'NECAP2',\n",
       " 'RP4-798A10.2',\n",
       " 'RP4-798A10.7',\n",
       " 'RP4-798A10.4',\n",
       " 'RP5-875O13.1',\n",
       " 'FAM231C',\n",
       " 'NBPF1',\n",
       " 'RP5-1182A14.5',\n",
       " 'RP1-163M9.8',\n",
       " 'FAM231A',\n",
       " 'RP1-163M9.7',\n",
       " 'FAM231C-1',\n",
       " 'RP5-1182A14.7',\n",
       " 'CROCC',\n",
       " 'RP11-108M9.1',\n",
       " 'RP11-108M9.2',\n",
       " 'RP11-108M9.3',\n",
       " 'RP11-108M9.4',\n",
       " 'RP11-108M9.6',\n",
       " 'MFAP2',\n",
       " 'RP1-37C10.3',\n",
       " 'ATP13A2',\n",
       " 'SDHB',\n",
       " 'PADI2',\n",
       " 'RP11-380J14.1',\n",
       " 'RP11-380J14.4',\n",
       " 'PADI1',\n",
       " 'PADI3',\n",
       " 'PADI4',\n",
       " 'AC004824.2',\n",
       " 'PADI6',\n",
       " 'RP1-20B21.4',\n",
       " 'RCC2',\n",
       " 'ARHGEF10L',\n",
       " 'RP11-473A10.2',\n",
       " 'ACTL8',\n",
       " 'RP11-174G17.2',\n",
       " 'IGSF21',\n",
       " 'RP11-174G17',\n",
       " 'RP11-422P22.1',\n",
       " 'KLHDC7A',\n",
       " 'PAX7',\n",
       " 'TAS1R2',\n",
       " 'RP13-279N23.2',\n",
       " 'ALDH4A1',\n",
       " 'IFFO2',\n",
       " 'UBR4',\n",
       " 'RP1-43E13.2',\n",
       " 'EMC1',\n",
       " 'MRTO4',\n",
       " 'AKR7A3',\n",
       " 'AKR7A2',\n",
       " 'PQLC2',\n",
       " 'CAPZB',\n",
       " 'RP5-1056L3.1',\n",
       " 'MINOS1',\n",
       " 'MINOS1-NBL1',\n",
       " 'NBL1',\n",
       " 'HTR6',\n",
       " 'TMCO4',\n",
       " 'RNF186',\n",
       " 'RP11-91K11.2',\n",
       " 'OTUD3',\n",
       " 'PLA2G2E',\n",
       " 'PLA2G2A',\n",
       " 'PLA2G5',\n",
       " 'PLA2G2D',\n",
       " 'PLA2G2F',\n",
       " 'RP3-340N1.2',\n",
       " 'PLA2G2C',\n",
       " 'UBXN10-AS1',\n",
       " 'UBXN10',\n",
       " 'RP3-340N1.6',\n",
       " 'VWA5B1',\n",
       " 'RP4-745E8.2',\n",
       " 'LINC01141',\n",
       " 'RP4-749H3.2',\n",
       " 'CAMK2N1',\n",
       " 'MUL1',\n",
       " 'FAM43B',\n",
       " 'CDA',\n",
       " 'PINK1',\n",
       " 'PINK1-AS',\n",
       " 'DDOST',\n",
       " 'KIF17',\n",
       " 'SH2D5',\n",
       " 'RP5-930J4.2',\n",
       " 'HP1BP3',\n",
       " 'EIF4G3',\n",
       " 'ECE1',\n",
       " 'RP3-329E20.2',\n",
       " 'RP5-1071N3.1',\n",
       " 'RP11-293F5.1',\n",
       " 'NBPF3',\n",
       " 'ALPL',\n",
       " 'RP11-63N8.3',\n",
       " 'RAP1GAP',\n",
       " 'USP48',\n",
       " 'LDLRAD2',\n",
       " 'HSPG2',\n",
       " 'CELA3B',\n",
       " 'CELA3A',\n",
       " 'RP1-224A6.3',\n",
       " 'LINC00339',\n",
       " 'CDC42',\n",
       " 'RP1-224A6.9',\n",
       " 'WNT4',\n",
       " 'ZBTB40',\n",
       " 'EPHA8',\n",
       " 'C1QA',\n",
       " 'C1QC',\n",
       " 'C1QB',\n",
       " 'EPHB2',\n",
       " 'RP11-69E9.1',\n",
       " 'LACTBL1',\n",
       " 'C1orf234',\n",
       " 'KDM1A',\n",
       " 'RP1-184J9.2',\n",
       " 'LUZP1',\n",
       " 'HTR1D',\n",
       " 'LINC01355',\n",
       " 'HNRNPR',\n",
       " 'ZNF436',\n",
       " 'ZNF436-AS1',\n",
       " 'RP5-1057J7.7',\n",
       " 'TCEA3',\n",
       " 'ASAP3',\n",
       " 'E2F2',\n",
       " 'RP1-150O5.3',\n",
       " 'ID3',\n",
       " 'MDS2',\n",
       " 'RPL11',\n",
       " 'TCEB3',\n",
       " 'TCEB3-AS1',\n",
       " 'PITHD1',\n",
       " 'LYPLA2',\n",
       " 'GALE',\n",
       " 'HMGCL',\n",
       " 'FUCA1',\n",
       " 'CNR2',\n",
       " 'RP11-4M23.3',\n",
       " 'PNRC2',\n",
       " 'SRSF10',\n",
       " 'RP11-293P20.2',\n",
       " 'MYOM3',\n",
       " 'RP11-293P20.4',\n",
       " 'IL22RA1',\n",
       " 'control',\n",
       " 'ARID1A',\n",
       " 'BCORL1',\n",
       " 'FOSB',\n",
       " 'SET',\n",
       " 'KLF1',\n",
       " 'OSR2',\n",
       " 'BAK1',\n",
       " 'FOXA3',\n",
       " 'FOXL2',\n",
       " 'HES7',\n",
       " 'IRF1',\n",
       " 'IGDCC3',\n",
       " 'MAPK1',\n",
       " 'UBASH3B',\n",
       " 'CBL',\n",
       " 'PTPN12',\n",
       " 'MAP2K6',\n",
       " 'SAMD1',\n",
       " 'BCL2L11',\n",
       " 'UBASH3A',\n",
       " 'LYL1',\n",
       " 'IER5L',\n",
       " 'CDKN1C',\n",
       " 'CDKN1A',\n",
       " 'AHR',\n",
       " 'FEV',\n",
       " 'LHX1',\n",
       " 'SGK1',\n",
       " 'TBX3',\n",
       " 'C3orf72',\n",
       " 'TGFBR2',\n",
       " 'PRTG',\n",
       " 'HOXB9',\n",
       " 'CDKN1B',\n",
       " 'CSRNP1',\n",
       " 'ISL2',\n",
       " 'MAP2K3',\n",
       " 'MAP7D1',\n",
       " 'IKZF3',\n",
       " 'SNAI1',\n",
       " 'ELMSAN1',\n",
       " 'FOXF1',\n",
       " 'CEBPB',\n",
       " 'PTPN1',\n",
       " 'ZBTB25',\n",
       " 'DUSP9',\n",
       " 'SLC4A1',\n",
       " 'COL2A1',\n",
       " 'CLDN6',\n",
       " 'STIL',\n",
       " 'CNN1',\n",
       " 'MAML2',\n",
       " 'ZBTB10',\n",
       " 'ZC3HAV1',\n",
       " 'HOXC13',\n",
       " 'DLX2',\n",
       " 'RREB1',\n",
       " 'CELF2',\n",
       " 'HNF4A',\n",
       " 'ETS2',\n",
       " 'PTPN9',\n",
       " 'COL1A1',\n",
       " 'SLC6A9',\n",
       " 'KIF18B',\n",
       " 'KIF2C',\n",
       " 'CEBPE',\n",
       " 'RUNX1T1',\n",
       " 'TBX2',\n",
       " 'SPI1',\n",
       " 'CNNM4',\n",
       " 'PLK4',\n",
       " 'CBFA2T3',\n",
       " 'BPGM',\n",
       " 'TMSB4X',\n",
       " 'CITED1',\n",
       " 'FOXA1',\n",
       " 'TSC22D1',\n",
       " 'HOXA13',\n",
       " 'CEBPA',\n",
       " 'ZNF318',\n",
       " 'C19orf26',\n",
       " 'MAP4K3',\n",
       " 'GLB1L2',\n",
       " 'KIAA1804',\n",
       " 'RHOXF2',\n",
       " 'MEIS1',\n",
       " 'MAP4K5',\n",
       " 'ARRDC3',\n",
       " 'SLC38A2',\n",
       " 'MIDN',\n",
       " 'ZBTB1',\n",
       " 'PRDM1',\n",
       " 'KMT2A',\n",
       " 'S1PR2',\n",
       " 'ATL1',\n",
       " 'CKS1B',\n",
       " 'PTPN13',\n",
       " 'EGR1',\n",
       " 'HK2',\n",
       " 'NCL',\n",
       " 'NIT1',\n",
       " 'POU3F2',\n",
       " 'FOXO4',\n",
       " 'JUN']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gene_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gene_lst.index(\"control\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
