{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "n_embed = 256\n",
    "n_genes = 500\n",
    "batch_size = 64\n",
    "time_dim = 10\n",
    "n_heads = 8\n",
    "dropout = 0.2\n",
    "n_perts = 2\n",
    "n_blocks = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import scanpy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "adata = sc.read(\"/Users/atamb/Downloads/NormanWeissman2019_filtered.h5ad\")\n",
    "# normalise data\n",
    "sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "# log transform the data\n",
    "sc.pp.log1p(adata)\n",
    "\n",
    "adata_subset = adata[:,:n_genes]\n",
    "# prep dataset\n",
    "adata_subset.X\n",
    "\n",
    "dataset = torch.tensor(adata_subset.X.toarray())\n",
    "top_vals, top_inds = torch.topk(dataset, time_dim, dim=1)\n",
    "top_inds = top_inds.to(device=\"mps\")\n",
    "\n",
    "\n",
    "# top_inds.to(device=\"mps\")\n",
    "\n",
    "gene_lst = list(adata_subset.var_names)\n",
    "gene_lst.append(\"control\")\n",
    "\n",
    "pertlst = list(adata_subset.obs.perturbation)\n",
    "len(pertlst)\n",
    "\n",
    "pert_dataset = []\n",
    "\n",
    "def gene_encoder(x):\n",
    "    x = x.split(\"_\")\n",
    "    if len(x) == 1:\n",
    "        # add one in order to account for mask being at index 0\n",
    "        return [gene_lst.index(x[0]) + 1, gene_lst.index(\"control\") + 1]\n",
    "    else:\n",
    "        return [gene_lst.index(x[0]) + 1, gene_lst.index(x[1]) + 1]\n",
    "\n",
    "\n",
    "for pert in pertlst:\n",
    "    ls = pert.split(\"_\")\n",
    "    if ls[0] not in gene_lst:\n",
    "        gene_lst.append(ls[0])\n",
    "    if len(ls) > 1:\n",
    "        if ls[1] not in gene_lst:\n",
    "            gene_lst.append(ls[1])\n",
    "    \n",
    "    pert_dataset.append(gene_encoder(pert))\n",
    "\n",
    "    # print(gene_encoder(pert))\n",
    "\n",
    "pert_dataset_tensor = torch.tensor(pert_dataset, device=\"mps\")\n",
    "\n",
    "n_genes = len(gene_lst)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_batch():\n",
    "    index_tensor = torch.randint(0, len(pert_dataset_tensor), (batch_size,), device=\"mps\")\n",
    "    # print(index_tensor)\n",
    "    perts = pert_dataset_tensor[index_tensor]\n",
    "    resp = top_inds[index_tensor]\n",
    "    return perts, resp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# block_sizes = [128, 64, 32, 16]\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, n_embed, head_size):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        self.batch_qkv_matrices = nn.Linear(n_embed, head_size * n_heads * 3, bias=False) \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        q,k,v = self.batch_qkv_matrices(x).split(self.head_size * n_heads, dim=-1) # Now Q,K,V of dim B, T, head size * n_heads\n",
    "       \n",
    "        B,T,C = x.shape\n",
    " \n",
    "        # reshape to B, T, n_heads, head_size\n",
    "        k = k.view(B, T, n_heads, self.head_size).transpose(1,2)\n",
    "        q = q.view(B, T, n_heads, self.head_size).transpose(1,2) # Now of shape B, n_heads, T, head_size for BMM\n",
    "        v = v.view(B, T, n_heads, self.head_size).transpose(1,2)\n",
    "   \n",
    "        # attention mechanism core\n",
    "        weight_mat = q @ k.transpose(-2, -1)\n",
    "        weight_mat = weight_mat * (self.head_size ** -0.5) #\n",
    "        weight_mat = F.softmax(weight_mat, dim=-1)\n",
    "\n",
    "        # regularisation\n",
    "        weight_mat = self.dropout(weight_mat)\n",
    "\n",
    "        # Multiply with values\n",
    "        res = weight_mat @ v\n",
    "\n",
    "        # post-processing\n",
    "        res = res.transpose(1,2) # B, n_heads, T, C --> B, T, n_heads, C   \n",
    "        res = res.contiguous().view(B, T, C)\n",
    "\n",
    "        return res\n",
    "\n",
    "\n",
    "class MHAttention(nn.Module):\n",
    "    def __init__(self, n_embed, head_size):\n",
    "        super().__init__()\n",
    "        self.att_heads = Head(n_embed=n_embed, head_size=head_size)\n",
    "        self.projection = nn.Linear(n_embed, n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        res = self.att_heads(x)\n",
    "        res = self.dropout(self.projection(res))\n",
    "        return res \n",
    "\n",
    "class Feedforward(nn.Module):\n",
    "    def __init__(self, n_embed) -> None:\n",
    "        super().__init__()\n",
    "        scale_factor = 6\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(n_embed, n_embed * scale_factor),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_embed * scale_factor, n_embed),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_embed, n_embed*scale_factor),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_embed*scale_factor, n_embed),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ff(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embed, n_heads) -> None:\n",
    "        super().__init__()\n",
    "        self.ff = Feedforward(n_embed)\n",
    "        self.mhatt = MHAttention(n_embed, (n_embed // n_heads))\n",
    "        self.layer_norm1 = nn.LayerNorm(n_embed) \n",
    "        self.layer_norm2 = nn.LayerNorm(n_embed)\n",
    "    def forward(self, x):\n",
    "        x = x + self.mhatt(self.layer_norm1(x))\n",
    "        x = x + self.ff(self.layer_norm2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "    \n",
    "# Bidirectional Encoder representations from transformers for RNA-seq (BERNA)\n",
    "class BERNA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERNA, self).__init__()\n",
    "        # 0th embedding is for mask!!!\n",
    "        self.embed_table = nn.Embedding(n_genes+1, n_embed)\n",
    "        # two embeddings, unique for both perts and responses\n",
    "        self.pos_embed = nn.Embedding(2, n_embed)\n",
    "        # we want to just keep the indices for positional embeddings stored so we don't create a new tensor every time\n",
    "        self.register_buffer(\"perts_pos_embed\", torch.zeros(size=(batch_size,n_perts), device=\"mps\").long())\n",
    "        self.register_buffer(\"responses_pos_embed\", torch.ones(size=(batch_size,time_dim), device=\"mps\").long())\n",
    "\n",
    "        # Attention blocks\n",
    "        self.blocks = nn.Sequential(*[Block(n_embed, n_heads) for _ in range(n_blocks)])\n",
    "        self.layernorm = nn.LayerNorm(n_embed)\n",
    "        self.final_proj = nn.Linear(n_embed, n_genes)\n",
    "\n",
    "\n",
    "    def forward(self,perts, responses):\n",
    "        x = torch.cat([perts, responses], dim=-1) # of dimension (B, 12). 12 because 2 perts and 10 responses per batch element\n",
    "        # get ones, then find random positions to mask, set to zero, then multiply\n",
    "        mask = torch.ones_like(x)\n",
    "        rand_pos = torch.randint(0, time_dim + n_perts, size=(batch_size, ), device=\"mps\")\n",
    "        \n",
    "        target_token = x[torch.arange(batch_size), rand_pos]         \n",
    "\n",
    "        mask[torch.arange(x.size(0)), rand_pos] = 0\n",
    "   \n",
    "        x = x * mask\n",
    "\n",
    "        # re-split after masking into perts and responses\n",
    "        perts_mod = x[:, :n_perts]\n",
    "        responses_mod = x[:, n_perts:]\n",
    "\n",
    "        # get perturbation embeddings\n",
    "        perts_embed = self.embed_table(perts_mod) + self.pos_embed(self.perts_pos_embed)\n",
    "\n",
    "        # get response embeddings\n",
    "        responses_embed = self.embed_table(responses_mod) + self.pos_embed(self.responses_pos_embed)\n",
    "\n",
    "        # re-concatenate and pass through blocks\n",
    "        x = torch.cat([perts_embed, responses_embed], dim=1)\n",
    "        x = self.final_proj(self.layernorm(self.blocks(x)))\n",
    "\n",
    "        # get logits for masked positions        \n",
    "        logits = x[torch.arange(batch_size), rand_pos]\n",
    "\n",
    "        loss = F.cross_entropy(logits, target_token)        \n",
    "        return loss        \n",
    "\n",
    "\n",
    "# b = BERNA().to(device=\"mps\")\n",
    "# test_perts = torch.randint(0, n_genes, (batch_size,2,)).to(device=\"mps\")\n",
    "# test_responses = torch.randint(0, n_genes, (batch_size,10)).to(device=\"mps\")\n",
    "\n",
    "# resp = b(test_perts, test_responses)\n",
    "\n",
    "# resp.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERNA().to(device=\"mps\")\n",
    "optim = torch.optim.Adam(model.parameters(), lr=3e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.436053276062012\n",
      "3.87002788066864\n",
      "2.5784301781654357\n",
      "2.2551401090621948\n",
      "2.1555699348449706\n",
      "2.209217324256897\n",
      "2.1970620560646057\n",
      "2.1959004855155944\n",
      "2.155860161781311\n",
      "2.1045082664489745\n",
      "2.0870564794540405\n",
      "2.105900583267212\n",
      "2.1085131669044497\n",
      "2.151959528923035\n",
      "2.114898271560669\n",
      "2.100181882381439\n",
      "2.0332925009727476\n",
      "2.1372216892242433\n",
      "2.0057388377189636\n",
      "2.0102581429481505\n",
      "2.060205237865448\n",
      "2.0840709137916567\n",
      "2.03759836435318\n",
      "2.0780045223236083\n",
      "2.009292631149292\n",
      "1.9633903074264527\n",
      "2.0514724683761596\n",
      "2.0436591863632203\n",
      "1.9800955486297607\n",
      "1.9493935227394104\n",
      "2.0009888863563536\n",
      "1.9492456603050232\n",
      "1.9421072936058044\n",
      "1.9988496494293213\n",
      "1.9725536632537841\n",
      "1.9248845195770263\n",
      "1.9107662296295167\n",
      "1.951020085811615\n",
      "1.935927131175995\n",
      "1.9327314972877503\n",
      "1.8982454490661622\n",
      "1.9213224864006042\n",
      "1.942727143764496\n",
      "1.9726701283454895\n",
      "1.927194182872772\n",
      "1.9363679385185242\n",
      "1.8983551263809204\n",
      "1.9042508149147033\n",
      "1.9356699061393738\n",
      "1.8861456489562989\n",
      "1.9115185022354126\n",
      "1.8997963905334472\n",
      "1.8955616855621338\n",
      "1.853654396533966\n",
      "1.9749171757698059\n",
      "1.9802153563499452\n",
      "1.8755139446258544\n",
      "1.9169136714935302\n",
      "1.9039808368682862\n",
      "1.9536578965187072\n",
      "1.9011132669448854\n",
      "1.8621933221817017\n",
      "1.9234954309463501\n",
      "1.9643731665611268\n",
      "1.8930435657501221\n",
      "1.9231583452224732\n",
      "1.8888618087768554\n",
      "1.9051015949249268\n",
      "1.8731048464775086\n",
      "1.8901730728149415\n",
      "1.9367057514190673\n",
      "1.8436695837974548\n",
      "1.9118702507019043\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5000\n",
    "\n",
    "loss_lst = []\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    optim.zero_grad()\n",
    "    perts, resp = gen_batch()\n",
    "    loss = model(perts, resp)\n",
    "    loss.backward()\n",
    "    loss_lst.append(loss.item())\n",
    "    if i % 50 == 0:\n",
    "        print(sum(loss_lst) / len(loss_lst))\n",
    "        loss_lst = []\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
